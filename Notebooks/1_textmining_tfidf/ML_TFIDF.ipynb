{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition:\n",
    "\n",
    "This will apply TF and IDF on posts collected from stackoverflow in order to determine what is the topic of the post.\n",
    "\n",
    "## Problem usage\n",
    "\n",
    "Using this model , when you see a new question, you can tell which category it is talking about. So, usage: text classification.\n",
    "\n",
    "Tutorial followed:\n",
    "http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XBuSavmnGUn.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset Loaded in 1.680s.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "n_features=10000 #number of words/features to consider from all documents\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "# read json into a dataframe\n",
    "df= pd.read_json(\"data/stackoverflow-data-idf.json\",lines=True)\n",
    "print(\"Dataset Loaded in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                          20000\n",
       "title                       20000\n",
       "body                        20000\n",
       "answer_count                20000\n",
       "comment_count               20000\n",
       "creation_date               20000\n",
       "last_activity_date          20000\n",
       "last_editor_display_name    20000\n",
       "owner_display_name          20000\n",
       "owner_user_id               19762\n",
       "post_type_id                20000\n",
       "score                       20000\n",
       "tags                        20000\n",
       "view_count                  20000\n",
       "accepted_answer_id          10711\n",
       "favorite_count               4471\n",
       "last_edit_date              10708\n",
       "last_editor_user_id         10595\n",
       "community_owned_date           15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset understanding\n",
    "\n",
    "In this example, we are using a Stackoverflow dataset which is slightly noisier and simulates what you could be dealing with in real life. You can find this dataset in the tutorial http://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.XBuSavmnGUn.\n",
    "\n",
    "Notice that there are two files, the larger file with (20,000 posts)[https://github.com/kavgan/data-science-tutorials/tree/master/tf-idf/data] is used to compute the Inverse Document Frequency (IDF) and the smaller file with 500 posts would be used as a test set for us to extract keywords from. This dataset is based on the publicly available Stackoverflow dump on Google's Big Query.\n",
    "\n",
    "Let's take a peek at our dataset. The code below reads a one per line json string from data/stackoverflow-data-idf.json into a pandas data frame and prints out its schema and total number of posts. Here, lines=True simply means we are treating each line in the text file as a separate json string. With this, the json in line 1 is not related to the json in line 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epreraw\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>last_editor_display_name</th>\n",
       "      <th>owner_display_name</th>\n",
       "      <th>owner_user_id</th>\n",
       "      <th>post_type_id</th>\n",
       "      <th>score</th>\n",
       "      <th>tags</th>\n",
       "      <th>view_count</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>last_editor_user_id</th>\n",
       "      <th>community_owned_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4821394</td>\n",
       "      <td>Serializing a private struct - Can it be done?</td>\n",
       "      <td>&lt;p&gt;I have a public class that contains a private struct. The struct contains properties (mostly string) that I want to serialize. When I attempt to serialize the struct and stream it to disk, using XmlSerializer, I get an error saying only public types can be serialized. I don't need, and don't want, this struct to be public. Is there a way I can serialize it and keep it private?&lt;/p&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-27 20:19:13.563 UTC</td>\n",
       "      <td>2011-01-27 20:21:37.59 UTC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>163534.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c#|serialization|xml-serialization</td>\n",
       "      <td>296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3367882</td>\n",
       "      <td>How do I prevent floated-right content from overlapping main content?</td>\n",
       "      <td>&lt;p&gt;I have the following HTML:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;lt;td class='a'&amp;gt;\\n  &amp;lt;img src='/images/some_icon.png' alt='Some Icon' /&amp;gt;\\n  &amp;lt;span&amp;gt;Some content that's waaaaaaaaay too long to fit in the allotted space, but which can get cut off.&amp;lt;/span&amp;gt;\\n&amp;lt;/td&amp;gt;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;It should display as follows:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;[Some content that's wa [ICON]]\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I have the following CSS:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;td.a span {\\n  overflow: hidden;\\n  white-space: nowrap;\\n  z-index: 1;\\n}\\n\\ntd.a img {\\n  display: block;\\n  float: right;\\n  z-index: 2;\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;When I resize the browser to cut off the text, it cuts off at the edge of the &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt; rather than before the &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt;, which leaves the &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; overlapping the &lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt; content. I've tried various &lt;code&gt;padding&lt;/code&gt; and &lt;code&gt;margin&lt;/code&gt;s, but nothing seemed to work. Is this possible?&lt;/p&gt;\\n\\n&lt;p&gt;NB: It's &lt;em&gt;very&lt;/em&gt; difficult to add a &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt; that just contains the &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; here. If it were easy, I'd just do that :)&lt;/p&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2010-07-30 00:01:50.9 UTC</td>\n",
       "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1190.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>css|overflow|css-float|crop</td>\n",
       "      <td>4121</td>\n",
       "      <td>3367943.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
       "      <td>44390.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  4821394   \n",
       "1  3367882   \n",
       "\n",
       "                                                                   title  \\\n",
       "0  Serializing a private struct - Can it be done?                          \n",
       "1  How do I prevent floated-right content from overlapping main content?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    body  \\\n",
       "0  <p>I have a public class that contains a private struct. The struct contains properties (mostly string) that I want to serialize. When I attempt to serialize the struct and stream it to disk, using XmlSerializer, I get an error saying only public types can be serialized. I don't need, and don't want, this struct to be public. Is there a way I can serialize it and keep it private?</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "1  <p>I have the following HTML:</p>\\n\\n<pre><code>&lt;td class='a'&gt;\\n  &lt;img src='/images/some_icon.png' alt='Some Icon' /&gt;\\n  &lt;span&gt;Some content that's waaaaaaaaay too long to fit in the allotted space, but which can get cut off.&lt;/span&gt;\\n&lt;/td&gt;\\n</code></pre>\\n\\n<p>It should display as follows:</p>\\n\\n<pre><code>[Some content that's wa [ICON]]\\n</code></pre>\\n\\n<p>I have the following CSS:</p>\\n\\n<pre><code>td.a span {\\n  overflow: hidden;\\n  white-space: nowrap;\\n  z-index: 1;\\n}\\n\\ntd.a img {\\n  display: block;\\n  float: right;\\n  z-index: 2;\\n}\\n</code></pre>\\n\\n<p>When I resize the browser to cut off the text, it cuts off at the edge of the <code>&lt;td&gt;</code> rather than before the <code>&lt;img&gt;</code>, which leaves the <code>&lt;img&gt;</code> overlapping the <code>&lt;span&gt;</code> content. I've tried various <code>padding</code> and <code>margin</code>s, but nothing seemed to work. Is this possible?</p>\\n\\n<p>NB: It's <em>very</em> difficult to add a <code>&lt;td&gt;</code> that just contains the <code>&lt;img&gt;</code> here. If it were easy, I'd just do that :)</p>   \n",
       "\n",
       "   answer_count  comment_count                creation_date  \\\n",
       "0  1             0              2011-01-27 20:19:13.563 UTC   \n",
       "1  2             2              2010-07-30 00:01:50.9 UTC     \n",
       "\n",
       "            last_activity_date last_editor_display_name owner_display_name  \\\n",
       "0  2011-01-27 20:21:37.59 UTC                                                \n",
       "1  2012-05-10 14:16:05.143 UTC                                               \n",
       "\n",
       "   owner_user_id  post_type_id  score                                tags  \\\n",
       "0  163534.0       1             0      c#|serialization|xml-serialization   \n",
       "1  1190.0         1             2      css|overflow|css-float|crop          \n",
       "\n",
       "   view_count  accepted_answer_id  favorite_count  \\\n",
       "0  296        NaN                 NaN               \n",
       "1  4121        3367943.0           0.0              \n",
       "\n",
       "                last_edit_date  last_editor_user_id community_owned_date  \n",
       "0  NaN                         NaN                   NaN                  \n",
       "1  2012-05-10 14:16:05.143 UTC  44390.0              NaN                  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Understanding the data\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column datatypes:\n",
      "\n",
      " id                          int64  \n",
      "title                       object \n",
      "body                        object \n",
      "answer_count                int64  \n",
      "comment_count               int64  \n",
      "creation_date               object \n",
      "last_activity_date          object \n",
      "last_editor_display_name    object \n",
      "owner_display_name          object \n",
      "owner_user_id               float64\n",
      "post_type_id                int64  \n",
      "score                       int64  \n",
      "tags                        object \n",
      "view_count                  int64  \n",
      "accepted_answer_id          float64\n",
      "favorite_count              float64\n",
      "last_edit_date              object \n",
      "last_editor_user_id         float64\n",
      "community_owned_date        object \n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Column datatypes:\\n\\n\", df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of posts (20000, 19)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of posts\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and cleaning\n",
    "- Remove digits and special characters etc in the data\n",
    "- Remove stop words from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How do I prevent floated-right content from overlapping main content?<p>I have the following HTML:</p>\\n\\n<pre><code>&lt;td class='a'&gt;\\n  &lt;img src='/images/some_icon.png' alt='Some Icon' /&gt;\\n  &lt;span&gt;Some content that's waaaaaaaaay too long to fit in the allotted space, but which can get cut off.&lt;/span&gt;\\n&lt;/td&gt;\\n</code></pre>\\n\\n<p>It should display as follows:</p>\\n\\n<pre><code>[Some content that's wa [ICON]]\\n</code></pre>\\n\\n<p>I have the following CSS:</p>\\n\\n<pre><code>td.a span {\\n  overflow: hidden;\\n  white-space: nowrap;\\n  z-index: 1;\\n}\\n\\ntd.a img {\\n  display: block;\\n  float: right;\\n  z-index: 2;\\n}\\n</code></pre>\\n\\n<p>When I resize the browser to cut off the text, it cuts off at the edge of the <code>&lt;td&gt;</code> rather than before the <code>&lt;img&gt;</code>, which leaves the <code>&lt;img&gt;</code> overlapping the <code>&lt;span&gt;</code> content. I've tried various <code>padding</code> and <code>margin</code>s, but nothing seemed to work. Is this possible?</p>\\n\\n<p>NB: It's <em>very</em> difficult to add a <code>&lt;td&gt;</code> that just contains the <code>&lt;img&gt;</code> here. If it were easy, I'd just do that :)</p>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# More in preprocess can be done like\n",
    "# eliminate all code sections, normalize the words to its root, etc, \n",
    "# but for simplicity we perform only some mild pre-processing.\n",
    "\n",
    "def pre_process(text):    \n",
    "    # lowercase\n",
    "    text=text.lower()    \n",
    "    # remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)    \n",
    "    # remove special characters (like parenthesis, dots etc) and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)    \n",
    "    return text\n",
    " \n",
    "df['text'] = df['title'] + df['body']\n",
    "#show the second 'text' just for fun\n",
    "df['text'][1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how do i prevent floated right content from overlapping main content p i have the following html p pre code lt gt lt gt lt gt some content that s waaaaaaaaay too long to fit in the allotted space but which can get cut off lt gt lt gt code pre p it should display as follows p pre code some content that s wa icon code pre p i have the following css p pre code td a span overflow hidden white space nowrap z index td a img display block float right z index code pre p when i resize the browser to cut off the text it cuts off at the edge of the code lt gt code rather than before the code lt gt code which leaves the code lt gt code overlapping the code lt gt code content i ve tried various code padding code and code margin code s but nothing seemed to work is this possible p p nb it s em very em difficult to add a code lt gt code that just contains the code lt gt code here if it were easy i d just do that p '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(lambda x:pre_process(x))\n",
    " \n",
    "#show the second 'text' just for fun\n",
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: ['x\\n', 'y\\n', 'your\\n', 'yours\\n', 'yourself\\n', 'yourselves\\n', 'you\\n', 'yond\\n', 'yonder\\n', 'yon\\n', 'ye\\n', 'yet\\n', 'z\\n', 'zillion\\n', 'j\\n', 'u\\n', 'umpteen\\n', 'usually\\n', 'us\\n', 'username\\n', 'uponed\\n', 'upons\\n', 'uponing\\n', 'upon\\n', 'ups\\n', 'upping\\n', 'upped\\n', 'up\\n', 'unto\\n', 'until\\n', 'unless\\n', 'unlike\\n', 'unliker\\n', 'unlikest\\n', 'under\\n', 'underneath\\n', 'use\\n', 'used\\n', 'usedest\\n', 'r\\n', 'rath\\n', 'rather\\n', 'rathest\\n', 'rathe\\n', 're\\n', 'relate\\n', 'related\\n', 'relatively\\n', 'regarding\\n', 'really\\n', 'res\\n', 'respecting\\n', 'respectively\\n', 'q\\n', 'quite\\n', 'que\\n', 'qua\\n', 'n\\n', 'neither\\n', 'neaths\\n', 'neath\\n', 'nethe\\n', 'nethermost\\n', 'necessary\\n', 'necessariest\\n', 'necessarier\\n', 'never\\n', 'nevertheless\\n', 'nigh\\n', 'nighest\\n', 'nigher\\n', 'nine\\n', 'noone\\n', 'nobody\\n', 'nobodies\\n', 'nowhere\\n', 'nowheres\\n', 'no\\n', 'noes\\n', 'nor\\n', 'nos\\n', 'no-one\\n', 'none\\n', 'not\\n', 'notwithstanding\\n', 'nothings\\n', 'nothing\\n', 'nathless\\n', 'natheless\\n', 't\\n', 'ten\\n', 'tills\\n', 'till\\n', 'tilled\\n', 'tilling\\n', 'to\\n', 'towards\\n', 'toward\\n', 'towardest\\n', 'towarder\\n', 'together\\n', 'too\\n', 'thy\\n', 'thyself\\n', 'thus\\n', 'than\\n', 'that\\n', 'those\\n', 'thou\\n', 'though\\n', 'thous\\n', 'thouses\\n', 'thoroughest\\n', 'thorougher\\n', 'thorough\\n', 'thoroughly\\n', 'thru\\n', 'thruer\\n', 'thruest\\n', 'thro\\n', 'through\\n', 'throughout\\n', 'throughest\\n', 'througher\\n', 'thine\\n', 'this\\n', 'thises\\n', 'they\\n', 'thee\\n', 'the\\n', 'then\\n', 'thence\\n', 'thenest\\n', 'thener\\n', 'them\\n', 'themselves\\n', 'these\\n', 'therer\\n', 'there\\n', 'thereby\\n', 'therest\\n', 'thereafter\\n', 'therein\\n', 'thereupon\\n', 'therefore\\n', 'their\\n', 'theirs\\n', 'thing\\n', 'things\\n', 'three\\n', 'two\\n', 'o\\n', 'oh\\n', 'owt\\n', 'owning\\n', 'owned\\n', 'own\\n', 'owns\\n', 'others\\n', 'other\\n', 'otherwise\\n', 'otherwisest\\n', 'otherwiser\\n', 'of\\n', 'often\\n', 'oftener\\n', 'oftenest\\n', 'off\\n', 'offs\\n', 'offest\\n', 'one\\n', 'ought\\n', 'oughts\\n', 'our\\n', 'ours\\n', 'ourselves\\n', 'ourself\\n', 'out\\n', 'outest\\n', 'outed\\n', 'outwith\\n', 'outs\\n', 'outside\\n', 'over\\n', 'overallest\\n', 'overaller\\n', 'overalls\\n', 'overall\\n', 'overs\\n', 'or\\n', 'orer\\n', 'orest\\n', 'on\\n', 'oneself\\n', 'onest\\n', 'ons\\n', 'onto\\n', 'a\\n', 'atween\\n', 'at\\n', 'athwart\\n', 'atop\\n', 'afore\\n', 'afterward\\n', 'afterwards\\n', 'after\\n', 'afterest\\n', 'afterer\\n', 'ain\\n', 'an\\n', 'any\\n', 'anything\\n', 'anybody\\n', 'anyone\\n', 'anyhow\\n', 'anywhere\\n', 'anent\\n', 'anear\\n', 'and\\n', 'andor\\n', 'another\\n', 'around\\n', 'ares\\n', 'are\\n', 'aest\\n', 'aer\\n', 'against\\n', 'again\\n', 'accordingly\\n', 'abaft\\n', 'abafter\\n', 'abaftest\\n', 'abovest\\n', 'above\\n', 'abover\\n', 'abouter\\n', 'aboutest\\n', 'about\\n', 'aid\\n', 'amidst\\n', 'amid\\n', 'among\\n', 'amongst\\n', 'apartest\\n', 'aparter\\n', 'apart\\n', 'appeared\\n', 'appears\\n', 'appear\\n', 'appearing\\n', 'appropriating\\n', 'appropriate\\n', 'appropriatest\\n', 'appropriates\\n', 'appropriater\\n', 'appropriated\\n', 'already\\n', 'always\\n', 'also\\n', 'along\\n', 'alongside\\n', 'although\\n', 'almost\\n', 'all\\n', 'allest\\n', 'aller\\n', 'allyou\\n', 'alls\\n', 'albeit\\n', 'awfully\\n', 'as\\n', 'aside\\n', 'asides\\n', 'aslant\\n', 'ases\\n', 'astrider\\n', 'astride\\n', 'astridest\\n', 'astraddlest\\n', 'astraddler\\n', 'astraddle\\n', 'availablest\\n', 'availabler\\n', 'available\\n', 'aughts\\n', 'aught\\n', 'vs\\n', 'v\\n', 'variousest\\n', 'variouser\\n', 'various\\n', 'via\\n', 'vis-a-vis\\n', 'vis-a-viser\\n', 'vis-a-visest\\n', 'viz\\n', 'very\\n', 'veriest\\n', 'verier\\n', 'versus\\n', 'k\\n', 'g\\n', 'go\\n', 'gone\\n', 'good\\n', 'got\\n', 'gotta\\n', 'gotten\\n', 'get\\n', 'gets\\n', 'getting\\n', 'b\\n', 'by\\n', 'byandby\\n', 'by-and-by\\n', 'bist\\n', 'both\\n', 'but\\n', 'buts\\n', 'be\\n', 'beyond\\n', 'because\\n', 'became\\n', 'becomes\\n', 'become\\n', 'becoming\\n', 'becomings\\n', 'becominger\\n', 'becomingest\\n', 'behind\\n', 'behinds\\n', 'before\\n', 'beforehand\\n', 'beforehandest\\n', 'beforehander\\n', 'bettered\\n', 'betters\\n', 'better\\n', 'bettering\\n', 'betwixt\\n', 'between\\n', 'beneath\\n', 'been\\n', 'below\\n', 'besides\\n', 'beside\\n', 'm\\n', 'my\\n', 'myself\\n', 'mucher\\n', 'muchest\\n', 'much\\n', 'must\\n', 'musts\\n', 'musths\\n', 'musth\\n', 'main\\n', 'make\\n', 'mayest\\n', 'many\\n', 'mauger\\n', 'maugre\\n', 'me\\n', 'meanwhiles\\n', 'meanwhile\\n', 'mostly\\n', 'most\\n', 'moreover\\n', 'more\\n', 'might\\n', 'mights\\n', 'midst\\n', 'midsts\\n', 'h\\n', 'huh\\n', 'humph\\n', 'he\\n', 'hers\\n', 'herself\\n', 'her\\n', 'hereby\\n', 'herein\\n', 'hereafters\\n', 'hereafter\\n', 'hereupon\\n', 'hence\\n', 'hadst\\n', 'had\\n', 'having\\n', 'haves\\n', 'have\\n', 'has\\n', 'hast\\n', 'hardly\\n', 'hae\\n', 'hath\\n', 'him\\n', 'himself\\n', 'hither\\n', 'hitherest\\n', 'hitherer\\n', 'his\\n', 'how-do-you-do\\n', 'however\\n', 'how\\n', 'howbeit\\n', 'howdoyoudo\\n', 'hoos\\n', 'hoo\\n', 'w\\n', 'woulded\\n', 'woulding\\n', 'would\\n', 'woulds\\n', 'was\\n', 'want\\n', 'wast\\n', 'we\\n', 'wert\\n', 'were\\n', 'with\\n', 'withal\\n', 'without\\n', 'within\\n', 'why\\n', 'what\\n', 'whatever\\n', 'whateverer\\n', 'whateverest\\n', 'whatsoeverer\\n', 'whatsoeverest\\n', 'whatsoever\\n', 'whence\\n', 'whencesoever\\n', 'whenever\\n', 'whensoever\\n', 'when\\n', 'whenas\\n', 'whether\\n', 'wheen\\n', 'whereto\\n', 'whereupon\\n', 'wherever\\n', 'whereon\\n', 'whereof\\n', 'where\\n', 'whereby\\n', 'wherewithal\\n', 'wherewith\\n', 'whereinto\\n', 'wherein\\n', 'whereafter\\n', 'whereas\\n', 'wheresoever\\n', 'wherefrom\\n', 'which\\n', 'whichever\\n', 'whichsoever\\n', 'whilst\\n', 'while\\n', 'whiles\\n', 'whithersoever\\n', 'whither\\n', 'whoever\\n', 'whosoever\\n', 'whoso\\n', 'whose\\n', 'whomever\\n', 's\\n', 'syne\\n', 'syn\\n', 'shalling\\n', 'shall\\n', 'shalled\\n', 'shalls\\n', 'shoulding\\n', 'should\\n', 'shoulded\\n', 'shoulds\\n', 'she\\n', 'sayyid\\n', 'sayid\\n', 'said\\n', 'saider\\n', 'saidest\\n', 'same\\n', 'samest\\n', 'sames\\n', 'samer\\n', 'saved\\n', 'sans\\n', 'sanses\\n', 'sanserifs\\n', 'sanserif\\n', 'so\\n', 'soer\\n', 'soest\\n', 'sobeit\\n', 'someone\\n', 'somebody\\n', 'somehow\\n', 'some\\n', 'somewhere\\n', 'somewhat\\n', 'something\\n', 'sometimest\\n', 'sometimes\\n', 'sometimer\\n', 'sometime\\n', 'several\\n', 'severaler\\n', 'severalest\\n', 'serious\\n', 'seriousest\\n', 'seriouser\\n', 'senza\\n', 'send\\n', 'sent\\n', 'seem\\n', 'seems\\n', 'seemed\\n', 'seemingest\\n', 'seeminger\\n', 'seemings\\n', 'seven\\n', 'summat\\n', 'sups\\n', 'sup\\n', 'supping\\n', 'supped\\n', 'such\\n', 'since\\n', 'sine\\n', 'sines\\n', 'sith\\n', 'six\\n', 'stop\\n', 'stopped\\n', 'p\\n', 'plaintiff\\n', 'plenty\\n', 'plenties\\n', 'please\\n', 'pleased\\n', 'pleases\\n', 'per\\n', 'perhaps\\n', 'particulars\\n', 'particularly\\n', 'particular\\n', 'particularest\\n', 'particularer\\n', 'pro\\n', 'providing\\n', 'provides\\n', 'provided\\n', 'provide\\n', 'probably\\n', 'l\\n', 'layabout\\n', 'layabouts\\n', 'latter\\n', 'latterest\\n', 'latterer\\n', 'latterly\\n', 'latters\\n', 'lots\\n', 'lotting\\n', 'lotted\\n', 'lot\\n', 'lest\\n', 'less\\n', 'ie\\n', 'ifs\\n', 'if\\n', 'i\\n', 'info\\n', 'information\\n', 'itself\\n', 'its\\n', 'it\\n', 'is\\n', 'idem\\n', 'idemer\\n', 'idemest\\n', 'immediate\\n', 'immediately\\n', 'immediatest\\n', 'immediater\\n', 'in\\n', 'inwards\\n', 'inwardest\\n', 'inwarder\\n', 'inward\\n', 'inasmuch\\n', 'into\\n', 'instead\\n', 'insofar\\n', 'indicates\\n', 'indicated\\n', 'indicate\\n', 'indicating\\n', 'indeed\\n', 'inc\\n', 'f\\n', 'fact\\n', 'facts\\n', 'fs\\n', 'figupon\\n', 'figupons\\n', 'figuponing\\n', 'figuponed\\n', 'few\\n', 'fewer\\n', 'fewest\\n', 'frae\\n', 'from\\n', 'failing\\n', 'failings\\n', 'five\\n', 'furthers\\n', 'furtherer\\n', 'furthered\\n', 'furtherest\\n', 'further\\n', 'furthering\\n', 'furthermore\\n', 'fourscore\\n', 'followthrough\\n', 'for\\n', 'forwhy\\n', 'fornenst\\n', 'formerly\\n', 'former\\n', 'formerer\\n', 'formerest\\n', 'formers\\n', 'forbye\\n', 'forby\\n', 'fore\\n', 'forever\\n', 'forer\\n', 'fores\\n', 'four\\n', 'd\\n', 'ddays\\n', 'dday\\n', 'do\\n', 'doing\\n', 'doings\\n', 'doe\\n', 'does\\n', 'doth\\n', 'downwarder\\n', 'downwardest\\n', 'downward\\n', 'downwards\\n', 'downs\\n', 'done\\n', 'doner\\n', 'dones\\n', 'donest\\n', 'dos\\n', 'dost\\n', 'did\\n', 'differentest\\n', 'differenter\\n', 'different\\n', 'describing\\n', 'describe\\n', 'describes\\n', 'described\\n', 'despiting\\n', 'despites\\n', 'despited\\n', 'despite\\n', 'during\\n', 'c\\n', 'cum\\n', 'circa\\n', 'chez\\n', 'cer\\n', 'certain\\n', 'certainest\\n', 'certainer\\n', 'cest\\n', 'canst\\n', 'cannot\\n', 'can\\n', 'cant\\n', 'cants\\n', 'canting\\n', 'cantest\\n', 'canted\\n', 'co\\n', 'could\\n', 'couldst\\n', 'comeon\\n', 'comeons\\n', 'come-ons\\n', 'come-on\\n', 'concerning\\n', 'concerninger\\n', 'concerningest\\n', 'consequently\\n', 'considering\\n', 'e\\n', 'eg\\n', 'eight\\n', 'either\\n', 'even\\n', 'evens\\n', 'evenser\\n', 'evensest\\n', 'evened\\n', 'evenest\\n', 'ever\\n', 'everyone\\n', 'everything\\n', 'everybody\\n', 'everywhere\\n', 'every\\n', 'ere\\n', 'each\\n', 'et\\n', 'etc\\n', 'elsewhere\\n', 'else\\n', 'ex\\n', 'excepted\\n', 'excepts\\n', 'except\\n', 'excepting\\n', 'exes\\n', 'enough\\n', 'foo\\n', 'bar\\n', 'lt\\n', 'gt\\n', 'pre\\n', 'code\\n', 'blockquote\\n', 'li\\n', 'h1\\n', 'h2\\n', 'h3\\n', 'h4\\n', 'h5\\n', 'h6\\n', 'h7\\n', 'lt\\n', 'gt\\n', 'will\\n', 'being']\n",
      "Stopset: {'gone', 'oftener', 'ere', 'you', 'evens', 'outest', 'woulds', 'but', 'herein', 'atween', 'offest', 'almost', 'whatsoever', 'amidst', 'might', 'with', 'severalest', 'thereupon', 'inward', 'be', 'less', 'immediater', 'indicated', 'saved', 'latterest', 'afterward', 'few', 'withal', 'layabouts', 'nigh', 'aller', 'througher', 'in', 'whosoever', 'amongst', 'can', 'sanserif', 'b', 'despiting', 'awfully', 'most', 'us', 'lotted', 'qua', 'thing', 'four', 'downwarder', 'each', 'else', 'between', 'whithersoever', 'hereby', 'main', 'have', 'beforehandest', 'whencesoever', 'more', 'x', 'ever', 'versus', 'ex', 'furthers', 'failings', 'unless', 'sames', 'idemest', 'username', 'wherewith', 'therest', 'musts', 'wherefrom', 'had', 'yourselves', 'shalls', 'per', 'meanwhile', 'somewhere', 'fewer', 'was', 'aparter', 'thereby', 'ought', 'doe', 'thoroughest', 'provide', 'differentest', 'because', 'que', 'formerest', 'formerly', 'aught', 'shoulding', 'ours', 'abaft', 'vis-a-viser', 'become', 'nobody', 'or', 'overallest', 'whoso', 'downwardest', 'syn', 'since', 'h5', 'uponing', 'concerninger', 'relate', 'tilling', 'also', 'often', 'yours', 'afterest', 'despited', 'whereinto', 'appropriatest', 'whatsoeverer', 'seemed', 'plenties', 'without', 'y', 'go', 'fornenst', 'latters', 'whenas', 'bettered', 'send', 'h3', 'appropriater', 'howbeit', 'perhaps', 'come-on', 'what', 'tilled', 'wherein', 'sup', 'canst', 'unliker', 'circa', 'below', 'than', 'around', 'dost', 'not', 'former', 'downward', 'inwards', 'becomes', 'c', 'h2', 'anything', 'gt', 'indicating', 'said', 'so', 'ye', 'noes', 'lest', 'before', 'thence', 'nevertheless', 'h', 'wert', 'astraddlest', 'abafter', 'her', 'particularly', 'l', 'things', 'concerning', 'orer', 'sometimes', 'appropriate', 'appears', 'except', 'inasmuch', 'shalling', 'nighest', 'never', 'afterer', 'comeon', 'usedest', 'downwards', 'evened', 'yet', 'sans', 'thereafter', 'your', 'done', 'k', 'somebody', 'this', 'hitherer', 'rather', 'whoever', 'sent', 'doner', 'unlike', 'evenest', 'fores', 'appear', 'outs', 'howdoyoudo', 'outwith', 'andor', 'thorougher', 'rathest', 'much', 'excepts', 'fourscore', 'sanses', 'up', 'within', 'various', 'owns', 'hereafters', 'towarder', 'nethe', 'veriest', 'w', 'supping', 'astridest', 'different', 'nowheres', 'overalls', 'nine', 'abover', 'variouser', 'concerningest', 'sups', 'we', 'off', 'seemingest', 'cer', 'nos', 'information', 'couldst', 'hoos', 'for', 'thy', 'oneself', 'nor', 'betwixt', 'sith', 'cum', 'et', 'beyond', 'everyone', 'should', 'z', 'nobodies', 'use', 'neaths', 'too', 'same', 'lt', 'available', 'me', 'along', 'behind', 'otherwiser', 'alongside', 'vs', 'sanserifs', 'j', 'regarding', 'ifs', 'astraddle', 'insofar', 'figuponing', 'onest', 'an', 'such', 'gotta', 'which', 'certainer', 'besides', 'even', 'whatsoeverest', 'thru', 'particular', 'outside', 'others', 'vis-a-visest', 'nothings', 'whence', 'till', 'from', 'furtherest', 'hast', 'everybody', 'aid', 'shalled', 'aest', 'better', 'seems', 'asides', 'provided', 'whereof', 'moreover', 'hae', 'always', 'comeons', 'ons', 'buts', 'whomever', 'became', 'appropriating', 'thener', 'good', 'layabout', 'no-one', 'having', 'mights', 'upons', 'whereafter', 'sobeit', 'samest', 'some', 'li', 'despites', 'foo', 'afterwards', 'upping', 'whereon', 'yon', 'furthermore', 'he', 'respecting', 'hitherest', 'getting', 'offs', 'immediatest', 'whenever', 'seem', 'latterer', 'h7', 'differenter', 'inwardest', 'instead', 'evensest', 'if', 'being', 'facts', 'got', 'byandby', 'summat', 'really', 'serious', 'its', 'o', 'would', 'anyone', 'rathe', 'thro', 'appearing', 'becominger', 'hardly', 'otherwisest', 'aughts', 'how-do-you-do', 'hath', 'our', 'wheresoever', 'over', 'elsewhere', 'anybody', 'ourselves', 'failing', 'doings', 'described', 'anywhere', 'ups', 'hereupon', 'related', 'oh', 'figuponed', 'usually', 'towardest', 'g', 'enough', 'come-ons', 'sayid', 'why', 'all', 'whereas', 'lots', 'nathless', 'whichever', 'whichsoever', 'cantest', 'seriousest', 'n', 'therefore', 'gotten', 'however', 'neither', 't', 'idem', 'samer', 'certain', 'whereto', 'immediately', 'three', 'becomingest', 'it', 'vis-a-vis', 'any', 'five', 'yond', 'those', 'aboutest', 'thorough', 'exes', 'h4', 'saider', 'during', 'upon', 'apart', 'seriouser', 'mucher', 'thruest', 'are', 'excepting', 'indicate', 'were', 'several', 'mayest', 'hence', 'h1', 'beforehand', 'under', 'quite', 'none', 'excepted', 'accordingly', 'mauger', 'shoulded', 'throughest', 'availablest', 'blockquote', 'she', 'outed', 'pleases', 'abouter', 'stopped', 'latter', 'whatever', 'latterly', 'thoroughly', 'thouses', 'get', 'shoulds', 'thruer', 'anent', 'thee', 'has', 'onto', 'abovest', 'woulding', 'describe', 'when', 'one', 'sometimest', 'nigher', 'haves', 'etc', 'underneath', 'formers', 'ddays', 'somewhat', 'someone', 'everywhere', 'together', 'variousest', 'bar', 'yonder', 'wheen', 'cants', 'sines', 'very', 'describing', 'against', 'e', 'u', 'used', 'provides', 'fact', 'do', 'ases', 'inwarder', 'out', 'wherewithal', 'throughout', 'ares', 'whateverest', 'cest', 'neath', 'sine', 'unto', 'itself', 'them', 'there', 'hers', 'whose', 'sometime', 'furthering', 'nothing', 'atop', 'tills', 'forby', 'although', 'wherever', 'becomings', 'into', 'their', 'behinds', 'm', 'while', 'i', 'themselves', 'whiles', 'f', 'huh', 'his', 'owning', 'wast', 'info', 'unlikest', 'again', 'herself', 'appropriated', 'plaintiff', 'musth', 'dos', 'syne', 'forer', 'ten', 'oughts', 'will', 'upped', 'owned', 'severaler', 'furthered', 'own', 'res', 'umpteen', 'cant', 'another', 'the', 'bist', 'hoo', 'v', 'about', 'thyself', 'via', 'orest', 'toward', 'figupons', 'midst', 'whereby', 'sayyid', 'woulded', 'aside', 'betters', 'furtherer', 'becoming', 'doing', 'anyhow', 'astrider', 'further', 'already', 'beneath', 'natheless', 'somehow', 'oftenest', 'anear', 'my', 'consequently', 'make', 'above', 'respectively', 'want', 'providing', 'p', 'by-and-by', 'himself', 'fore', 'theirs', 'availabler', 'shall', 'maugre', 'whether', 'at', 'thine', 'appeared', 'abaftest', 'nethermost', 'ourself', 'been', 'both', 'by', 'indicates', 'could', 'appropriates', 'everything', 'many', 'probably', 'lotting', 'seven', 'something', 'immediate', 'beforehander', 'myself', 'muchest', 'zillion', 'therer', 'overs', 'particularest', 'forwhy', 'other', 'amid', 'hadst', 'these', 'whilst', 'among', 'whateverer', 'rath', 'seemings', 'pre', 'donest', 'allyou', 'overaller', 'nowhere', 'hither', 'two', 'uponed', 'lot', 'certainest', 'forever', 'either', 'supped', 'mostly', 'code', 'humph', 'eight', 'no', 'thou', 'pro', 'canted', 'viz', 'on', 'musths', 'plenty', 'formerer', 'midsts', 'is', 'despite', 'r', 'chez', 'describes', 'thises', 's', 'to', 'particularer', 'evenser', 'dday', 'dones', 're', 'beside', 'until', 'indeed', 'through', 'pleased', 'astride', 'afore', 'alls', 'thus', 'though', 'meanwhiles', 'senza', 'necessary', 'downs', 'necessariest', 'notwithstanding', 'overall', 'aslant', 'canting', 'whensoever', 'inc', 'bettering', 'towards', 'every', 'owt', 'q', 'd', 'athwart', 'sometimer', 'eg', 'after', 'does', 'doth', 'thous', 'stop', 'yourself', 'of', 'aer', 'fewest', 'cannot', 'figupon', 'where', 'saidest', 'frae', 'idemer', 'him', 'allest', 'verier', 'apartest', 'that', 'otherwise', 'particulars', 'forbye', 'soer', 'as', 'seeminger', 'gets', 'then', 'fs', 'a', 'whereupon', 'noone', 'therein', 'necessarier', 'hereafter', 'astraddler', 'followthrough', 'ie', 'six', 'they', 'h6', 'albeit', 'how', 'co', 'considering', 'relatively', 'ain', 'must', 'please', 'soest', 'thenest', 'and', 'whither', 'did'}\n"
     ]
    }
   ],
   "source": [
    "# Read common stopwords from a list\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words & return as immutable frozen set \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        print(\"Stopwords:\",stopwords)\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        print(\"Stopset:\", stop_set)\n",
    "        return frozenset(stop_set)\n",
    "    \n",
    "#load a set of stop words\n",
    "stopwords=get_stop_words(\"data/stopwords.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat each question as a document and form a corpus of documents. \n",
    "# Then count occurence of specific words in each document of the corpus.\n",
    "# Learn the vocabulary dictionary and return term-document matrix.\n",
    "\n",
    "corpus = df['text'].tolist()\n",
    "#corpus = [\"Trying to understand\", \"I understand well\",\"Whateber\",\"What\",\"so so\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Term frequency\n",
    "cv.fit_transform() creates the vocabulary and returns a term-document matrix. With this, each column in the matrix represents a word in the vocabulary, while each row represents the document in our dataset where the values in this case are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epreraw\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary of words in the question, \n",
    "# ignore words that appear in more than 85% of documents, \n",
    "# or in just one document\n",
    "# eliminate stop words\n",
    "# Using scikit TF-IDF algorithm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer(max_df=0.85,stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(corpus)\n",
    "# Stop word removal and also words having 85% occurence in the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121131\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\epreraw\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 10000)\n"
     ]
    }
   ],
   "source": [
    "## From here, we can see that the vocabulary is quite alot , so limit the vocab to 10,000\n",
    "# min_df=2 could be set but that will remove occurence of words once, \n",
    "# instead smooth_idf=True is used so each word occurs atleast once\n",
    "\n",
    "cv=CountVectorizer(max_df=0.85,stop_words=stopwords, max_features=n_features)\n",
    "word_count_vector= cv.fit_transform(corpus)\n",
    "print(word_count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serializing',\n",
       " 'private',\n",
       " 'struct',\n",
       " 'public',\n",
       " 'class',\n",
       " 'contains',\n",
       " 'properties',\n",
       " 'string',\n",
       " 'serialize',\n",
       " 'attempt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Printing the first 10 words in the vocabulary\n",
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only need to do this once, this is a list of features extracted\n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TF-IDF Frequency\n",
    "\n",
    "TODO: One improvement could be to divide the frequency with the size of the document in order to not penalize small documents.\n",
    "Also: Instead of Countvectorizer & then TfidfTransformer, we could have directly used TfidfVectorizer as well. But this gives better understanding, hence did like this.\n",
    "\n",
    "Now calculate TF-IDF frequency\n",
    " \n",
    "TF: Term Frequency, which measures how frequently a term occurs in a document. This was calculated earlier. \n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is in the whole corpus.While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\"An extremely important point\" to note here is that the IDF should always be based on a large corpora and should be representative of texts you would be using to extract keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test docs into a dataframe, concatenate title and body and then apply same preprocessing.\n",
    "\n",
    "df_test=pd.read_json(\"data/ml1/stackoverflow-test.json\",lines=True)\n",
    "df_test['text'] = df_test['title'] + df_test['body']\n",
    "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
    " \n",
    "# get test docs into a list\n",
    "corpus_test=df_test['text'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model to do the topic/keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the document that we want to extract keywords from:\n",
    "doc=corpus_test[0]\n",
    " \n",
    "#generate tf-idf for the new test document using the vocabulary and document frequencies (df) learned by fit\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "integrate war plugin for m eclipse into eclipse project p i set up a small web project with jsf and maven now i want to deploy on a tomcat server is there a possibility to automate that like a button in eclipse that automatically deploys the project to tomcat p p i read about a the a href http maven apache org plugins maven war plugin rel nofollow noreferrer maven war plugin a but i couldn t find a tutorial how to integrate that into my process eclipse m eclipse p p can you link me to help or try to explain it thanks p \n",
      "\n",
      "===Keywords===\n",
      "eclipse 0.49\n",
      "maven 0.451\n",
      "war 0.393\n",
      "plugin 0.265\n",
      "integrate 0.232\n",
      "tomcat 0.223\n",
      "project 0.197\n",
      "automate 0.13\n",
      "jsf 0.125\n",
      "possibility 0.121\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "We can see that the question was about eclipse and maven so th model is doing a great job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
